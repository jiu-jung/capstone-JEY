<!-- Template for PROJECT REPORT of CapstoneDesign 2025-2H, initially written by khyoo -->
<!-- 본 파일은 2025년도 컴공 졸업프로젝트의 <1차보고서> 작성을 위한 기본 양식입니다. -->
<!-- 아래에 "*"..."*" 표시는 italic체로 출력하기 위해서 사용한 것입니다. -->
<!-- "내용"에 해당하는 부분을 지우고, 여러분 과제의 내용을 작성해 주세요. -->

# Team-Info
| (1) 과제명 | Lightweight Part-Aware 3D Texture Editing Using 2D Stable Diffusion |
|:---  |---  |
| (2) 팀 번호 / 팀 이름 | 19-JEY |
| (3) 팀 구성원 | 이다예 (1917026): 리더, 아이디어 및 프로젝트 계획 관리, 2D-to-3D adaptation, 2D semantic segmentation 연구 및 개발, part segmentation 검증 <br> 이상은 (2171036): 팀원, 회의/면담 기록 정리 및 관리, Text to texture 연구 및 개발, 2D-to-3D adaptation, 2D semantic segmentation 검증 <br> 정지우 (2171046) : 팀원, 프로젝트 마일스톤 관리 및 공유 파일 관리, part segmentation 연구 및 개발, Text to texture 검증			 |
| (4) 팀 지도교수 | 김영준 교수 |
| (5) 과제 분류 | 연구 과제 |
| (6) 과제 키워드 | Semantic Part Segmentation, Controllable Diffusion, Texture generation  |
| (7) 과제 내용 요약 |  본 연구는 고사양 3D 모델링 기술 없이도 특정 부위의 텍스처를 정밀하게 편집할 수 있는 2D 기반의 경량화된 텍스처링 프로세스를 제안한다. ~~SyncDreamer~~ *Zero-1-to-3*를 활용하여 단일 이미지로부터 멀티뷰를 생성하고, ~~VLPart와 SAM~~ *Grounded-SAM*을 통해 2D 이미지에서 의미 기반 부위 분할을 수행한다. 이후 ~~Text2LIVE를 활용해~~ *생성된 마스크를 바탕으로 ControlNet-inpaint을 활용하여* 텍스트 조건에 맞는 부위별 텍스처 편집을 적용한다. ~~Aggregation 모듈을 통해 시점 간 마스크를 통합하고, refinement 단계에서 SyncDreamer 기반 정렬을 수행해 정확성을 높인다.~~ *Aggregation & Refinement 모듈을 통해 생성된 이미지의 불일치 및 오류를 감소시켜 정확성을 높인다.* 최종적으로 부위별 제어 정확도와 텍스처 품질을 동시에 확보하면서도 일반 사용자가 쉽게 접근할 수 있는 시스템을 구축하는 것을 목표로 한다. |

<br>

# Project-Summary
| 항목 | 내용 |
|:---  |---  |
| (1) 문제 정의 |   최근 XR, AR, VR 등 3D 환경 기반 기술의 수요가 증가함에 따라, 이를 구성하는 핵심 요소인 3D 모델의 중요성도 함께 부각되고 있다. 특히 사용자의 요구에 맞춘 3D 모델 생성 및 편집 기술이 활발히 연구되면서, 이를 상용화한 다양한 서비스도 등장하고 있다. NeRF나 Gaussian Splatting과 같은 최신 렌더링 기법의 적용으로 성능은 크게 향상되었지만, 여전히 텍스처 오류가 존재하며, 사용자의 세부 요구사항을 정확히 반영하는 데에는 한계가 있다. 이러한 문제는 텍스처 작업에 익숙한 전문가가 아니라면 해결하기 어렵다는 점에서, 일반 사용자에게는 높은 진입 장벽으로 작용한다. <br>   특히 실제 3D 응용 프로그램이나 애니메이션을 제작할 때는 모델 전체가 아닌 특정 부위 단위의 정밀한 제어나 상호작용이 필요하며, 이를 위해서는 3D 모델을 부위별로 분리(segment)하는 작업이 필수적이다. 그러나 이 과정 역시 고난이도의 작업으로, 대부분의 일반 사용자에게는 익숙하지 않으며 전문가의 도움이 요구된다. 최근에는 이러한 문제를 해결하기 위해 3D part segmentation 분야에서 다양한 연구가 이루어지고 있지만, 많은 연구들이 고사양 하드웨어와 복잡한 3D 데이터를 전제로 하기 때문에 상용화에는 여전히 제약이 따른다. <br>  이러한 배경 속에서, 우리는 일반 사용자도 손쉽게 사용할 수 있는 2D 기반 부위별 텍스처 편집 방식을 제안한다. 이는 기존의 복잡하고 고사양을 요구하는 3D 텍스처링 기법의 한계를 극복하고, 부위별 정밀 제어가 가능한 사용자 친화적인 솔루션으로서의 가능성을 가진다.  |
| (2) 기존연구와의 비교 | ![Image](https://github.com/user-attachments/assets/1700cbec-b09d-47ef-a0c0-9a82122fab4f)1. 객체 분할 (Object Segmentation) <br>기존 연구 1 – PointNet (Qi et al., 2017)<br>PointNet은 3D 포인트 클라우드를 입력으로 하여 부위 단위의 정밀한 분할을 수행한 대표적인 fully-supervised 방식이다. 이는 3D 구조를 직접 다루어 정확한 분석이 가능하다는 장점이 있지만, 라벨링 비용이 높고 도메인 특화 경향이 강하며 일반화에 어려움이 있다는 단점이 있다.<br>→ 우리 연구는 이러한 한계를 보완하기 위해 2D Vision Language Model(VLM)을 기반으로 하여 연산량이 적으며 빠르게 동작할 수 있도록 접근하였다.<br><br>2. 2D to 3D Adaptation<br>기존 연구 2 – PartSLIP (Liu et al., 2022)<br>PartSLIP은 2D에서 GLIP을 활용하여 부위 인식을 수행한 후, 다양한 시점에서 얻은 bounding box 결과를 집계하여 3D 포인트 클라우드에 대응시킨다. aggregation과 voting 알고리즘을 통해 신뢰도를 향상시킨 것이 특징이다.<br>→ 우리 연구는 PartSLIP의 voting 구조를 기반으로 하면서도, 신경망을 통해 가중치 예측을 강화한 PartSTAD 방식을 적용해 예측 신뢰성을 높였다.<br><br>3. 3D 텍스처 편집 (3D Texture Editing)<br>기존 연구 3 – TEXTure (Richardson et al., 2023), Text2Tex (Chen et al., 2023)<br>두 연구 모두 stable diffusion 기반의 depth-aware 모델을 활용하여 텍스트 프롬프트에 따라 다양한 시점에서 일관성 있는 텍스처 맵을 생성한다. 그러나 조명 정보가 반영된 텍스처 생성으로 인해 3D 환경에서는 그림자나 반사 등 문제로 품질 저하가 발생한다. 또한 부위별 조건을 부여할 경우 왜곡된 결과가 도출된다.<br>TEXTure를 실험하여, 텍스처 맵 생성 결과가 높은 완성도로 생성됨을 확인하였지만, 부위별 인식이 불가함을 확인하였다.<br>기존 연구 4 – Paint3D (Zeng et al., 2024), Paint-it (Kim et al., 2024)<br>두 연구는 PBR 호환 텍스처 생성을 통해 실제 렌더링 품질 문제를 해결한다. 특히 Paint3D는 depth-aware diffusion과 UV inpainting 기법을 조합하여 조명 없는 고해상도 텍스처 생성을 가능하게 한다.<br>우리는 Paint3D 실험을 통해 조명 문제 없는 성공적인 텍스처 생성 결과를 확인 했으나, 텍스트 프롬프트만으로 완전한 부위별 텍스처 생성에는 한계가 있음을 확인하였다.<br><br>→ 기존 연구 3, 기존 연구 4에 대해, 우리 연구는 텍스처 편집 단계 전 2D 기반 세그멘테이션 모델을 활용해 의미론적 파트 분할을 수행하여 부위별 인식을 통한 텍스처 편집이 가능하도록 하였다. |
| (3) 제안 내용 |  정의한 문제를 해결하기 위해 접근성이 높은 2D 기반의 부위별 텍스처링 프로세스를 제안한다. 이를 통해 사용자가 원하는 부위별 텍스처를 손쉽게 생성하고, 보다 향상된 사용자 경험을 제공하고자 한다.<br>  ~~SyncDreamer~~ *Zero-1-to-3*로 생성한 멀티뷰 이미지에 ~~SAM과 VLPart~~ *Grounded-SAM*를 이용해 부위별 마스크를 생성하고, ~~Text2LIVE 기반의~~ *ControlNet의 inpaint 기능을 통한* 텍스처 편집을 수행한 후, Aggregation module을 통해 정보를 통합하고 refinement 과정에서 2D-3D 간 시점 정렬 및 정확도 비교를 통해 최종 텍스처 결과물을 생성한다.<br>  이를 정리한 전체 파이프라인은 다음과 같으며, 각 단계에 대한 구체적 설명은 (5) 주요 기능 리스트에서 이어진다. <br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/architecture.png"><br>Input → *Text Prompt Parsing module →* ~~SyncDreamer~~ *Zero-1-to-3* → *Image Pre-processing module →* Segmentation ~~(VLPart + SAM)~~ *(Grounded-SAM)* → *Part Instance Information Aggregation Module → Mask aggregation module →* Texture Editing ~~(Text2LIVE)~~ *(ControlNet-inpaint)* → ~~Aggregation →~~ Refinement *→ 3D modeling*|
| (4) 기대효과 및 의의 |  고사양 3D 모델링 환경 없이도 정밀한 부위별 텍스처 편집이 가능하며, 사용자 친화적이면서도 높은 품질의 결과물을 생성할 수 있다. 이는 XR/VR 콘텐츠 제작, 사용자 맞춤형 가구 디자인, 메타버스 3D asset 제작 등에서 활용 가능하다. |
| (5) 주요 기능 리스트 |~~1. SyncDreamer: Multiview 생성<br>기존 연구들을 비교 분석한 결과, 텍스처 품질 측면에서 SyncDreamer가 가장 우수한 성능을 보였다. 따라서, Refinement 단계에서 사용할 단일 이미지 기반 3D 모델의 품질을 보장하기 위해 SyncDreamer를 채택한다.~~ <br>*1. Zero-1-to-3: Multiview 생성<br>기존 연구들을 비교 분석한 결과, 3D 생성을 위한 고해상도 멀티뷰를 수분 내에 생성할 수 있는 Zero-1-to-3가 가장 연구 목표에 적합한 결과를 만들어냈다. 특히 SyncDreamer 기반 결과를 분석한 결과 품질-속도 균형이 맞지 않아 가볍고 고화질이라는 연구 목표를 충족하지 않았다. 따라서 단일 이미지 기반 3D 모델 고품질을 보장하기 위해 Zero-1-to-3를 채택한다.* <br><br>~~2. VLPart + SAM: Segmentation<br>생성된 멀티뷰에 대해 2D 기반 세그멘테이션 모델을 활용해 의미론적 파트 분할을 수행한다.<br>• 1차 분할: VLPart를 활용하여 semantic 정보를 기반으로 초기 분할을 수행<br>• 2차 정제: SAM을 활용해 경계 분할 성능을 높여 전체 정확도를 향상<br>이러한 다중 분할 후 통합하는 과정을 통해 semantic 표현력과 경계의 정밀도를 동시에 확보한다.~~ <br>*2. Grounded-SAM(GroundingDINO+SAM): Segmentation<br>생성된 멀티뷰에 대해 2D 기반 세그멘테이션 모델을 활용해 의미론적 파트 분할을 수행한다. 다른 모델들 VLPart+SAM, GLIP+SAM과 비교한 결과, 해당 모델이 sementic 표현력과 정밀도를 높이면서 가장 범용적으로 분할할 수 있었다. 따라서 Grounded-SAM을 채택하여 Segmentation을 수행한다.* <br><br>~~3. Text2LIVE: Texture Editing<br>분할된 결과에 대해 Text2LIVE를 이용해 텍스처 편집을 수행한다.<br>이 단계에서는 입력 텍스트 정보에 따라 부위별로 자연스럽고 일관성 있는 텍스처를 생성한다.~~ <br>*3. ControlNet-inpaint: Texture Editing<br>분할된 결과로 생성된 마스크를 바탕으로 ControlNet의 inpaint 기능을 이용해 텍스처 편집을 수행한다. 이 단계에서는 마스크로 지정된 영역만을 텍스트 정보에 따라 새로운 이미지로 대체 생성하는 방식을 통해  자연스럽고 일관성 있는 텍스처를 생성한다.* <br><br>4. Aggregation & Refinement<br>기존 연구에서 지적되었듯이, 2D 정보만으로는 정확한 3D 형태 복원에 한계가 있다. 따라서, 3D 구조 정보를 최대한 보존하기 위해 2D-to-3D 적응(adaptation) 기법의 개선을 목표로 설정한다. 그러나, 복잡한 MLP나 과도한 모듈 추가는 연구 목적과 부합하지 않기 때문에, 전체 프로세스에 3D 정보를 무리하게 삽입하지 않고, 최종 Refinement 단계에서 3D-aware attention 기법을 도입한다. 해당 과정에서는 변형된 단일 이미지로 생성된 3D 모델의 다각도 이미지와, 변형된 멀티뷰 이미지들을 비교하여 텍스처를 보정한다. 또한, SDS loss 기반의 3D-aware attention을 활용하여 전체 객체 수준의 정확성과 품질을 향상시킨다.|

<br>
 
# Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 |- 기능 요구사항:<br>① ~~SyncDreamer~~ *Zero-1-to3 기반* 멀티뷰 생성<br>② ~~VLPart + SAM~~ *Grounded-SAM* 기반 2D segmentation<br>③ ~~Text2LIVE~~ *ControlNet-inpaint*를 활용한 부위별 텍스처 편집<br>④ Aggregation module 설계 및 해당 데이터 기반 3D rendering <br>⑤ 3D/2D 결합 refinement<br><br>- 데이터 요구사항:<br>가구 중심의 라벨링 이미지 (Pascal-Part, ADE20K 등 가공), 3D 렌더링 결과 비교용 ground truth|
| (2) 전체 시스템 구성 |- 모듈 구조:<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/architecture.png"><br>Input → *Text Prompt Parsing module →* ~~SyncDreamer~~ *Zero-1-to-3* → *Image Pre-processing module →* Segmentation ~~(VLPart + SAM)~~ *(Grounded-SAM)* → *Part Instance Information Aggregation Module → Mask aggregation module →* Texture Editing ~~(Text2LIVE)~~ *(ControlNet-inpaint)* → ~~Aggregation →~~ Refinement *→ 3D modeling*<br><br>*1. Text Prompt Parsing module: 사용자의 복합 입력으로부터 객체-부위 정보와, 부위별 스타일 정보 추출. 최종적으로는 2개의 json 파일 추출. Grounded-SAM에서 최적으로 처리되는 형식의 부위-분할 프롬프트, ControlNet의 input으로 처리되는 부위 객체별 스타일 맵핑 프롬프트. ollama를 활용한 프롬프트 엔지니어링 및 파이썬으로 구현.* <br><br>~~1. SyncDreamer: SyncDreamer를 활용해 단일 이미지 기반 멀티뷰 생성~~<br>*2. Zero-1-to-3: 사용자의 입력 중 하나인 이미지를 입력으로 받음. Zero-1-to-3를 활용해 단일 이미지 기반 멀티뷰 생성. 512x512 사이즈의 8개 뷰 통합 이미지 생성*<br><br>~~2. VLPart + SAM: VLPart → SAM 순으로 input 이미지에 텍스트 프롬프트에서 주어진 부위 정보를 기반으로 하여 분할을 수행하고, 해당 결과를 SAM을 이용하여 정교화~~<br>*3. Image Pre-processing module: 뷰마다 분할 작업을 위한 전처리. 512*512 크기의 8개 개별 이미지로 가공*<br>*4. Grounded-SAM: Grounded-SAM 을 활용해 부위 분할 수행*<br><br>*5. Part Instance Information Aggregation Module: 한번에 탐지되지 못하고 여러개로 나누어진 부위 객체들을 라벨 이름에 따라 하나의 part instance로 통합*<br><br>*6. Mask aggregation module: 5번 모듈을 통해 정리된 정보들을 통해 부위별 마스크 생성. 객체_mask.png 형식으로 생성. 이렇게 생성된 마스크 이미지는 1번 모듈에서 생성된 객체-스타일 맵핑 정보와 함께 ControlNet에서 사용*<br><br>~~3. Text2LIVE: 분할된 마스크를 기반으로, Text2LIVE를 활용해 부위별 텍스트 조건 기반 특정 부위 텍스처 편집~~<br>*7. ControlNet-inpaint: 분할된 마스크를 기반으로, ControlNet의 inpaint 기능을 활용해 특정 부위의 텍스처를 텍스트 조건으로 편집*<br><br>*8.* ~~4.~~ Aggregation & Refinement: 2D 단일 이미지로만 생성된 3D 형태와 편집과정을 거친 2D 멀티 이미지를 비교하여 2D-3D 사이에서 일어나는 불일치 및 오류를 감소시킴. SDS loss 와 3D aware attention을 이용.<br><br>- 외부 모듈:<br>~~SyncDreamer~~ *Zero-1-to-3* (multiview generation), *Grounded-SAM* (segmentation), ~~Text2LIVE~~ *ControlNet* (texture editing)|
| (3) 주요엔진 및 기능 설계 |~~- Multiview Generator: SyncDreamer를 통해 8-view 동기화 이미지 생성<br>- Segmenter: VLPart로 의미 정보 분할 후 SAM으로 경계 보정<br>- Texturing Module: Text2LIVE를 이용해 텍스트 기반 RGBA 편집<br>- Aggregation & Refinement: 다양한 시점 마스크 통합, SyncDreamer 시점 보정 활용~~<br>*1. Text Prompt Parsing module: 사용자의 복합 입력으로부터 객체-부위 정보와, 부위별 스타일 정보 추출. 최종적으로는 2개의 json 파일 추출. Grounded-SAM에서 최적으로 처리되는 형식의 부위-분할 프롬프트, ControlNet의 input으로 처리되는 부위 객체별 스타일 맵핑 프롬프트. ollama를 활용한 프롬프트 엔지니어링 및 파이썬으로 구현 완료*<br><br>*2. Image Pre-processing module: 뷰마다 분할 작업을 위한 전처리. 512x512 크기의 8개 개별 이미지로 가공. 파이썬 활용 구현 완료*<br><br>*3. Part Instance Information Aggregation Module: 한번에 탐지되지 못하고 여러개로 나누어진 부위 객체들을 라벨 이름에 따라 하나의 part instance로 통합. 파이썬 활용 구현 완료*<br><br>*4. Mask aggregation module: 3번 모듈을 통해 정리된 정보들을 통해 부위별 마스크 생성. 객체_mask.png 형식으로 생성. 이렇게 생성된 마스크 이미지는 1번 모듈에서 생성된 객체-스타일 맵핑 정보와 함께 ControlNet에서 사용. 파이썬 활용 구현 완료*<br><br>*5. Aggregation & Refinement: 2D 단일 이미지로만 생성된 3D 형태와 편집과정을 거친 2D 멀티 이미지를 비교하여 2D-3D 사이에서 일어나는 불일치 및 오류를 감소시킴. SDS loss 와 3D aware attention을 이용*<br>*-구현 중. 3D 활용 렌더링 방식에 문제가 발생하여 결과물을 아직 생성하지 못했고, refinement를 하지 못함.*|
| (4) 주요 기능의 구현 |- 부위 분할: <br>~~VLPart + SAM~~ *Grounded-SAM* 순차 적용하여 부위별 마스크 생성<br>![Image](https://github.com/user-attachments/assets/dcb7fe61-2c86-4cbe-bb20-acee58304aaa)<br>~~→ 사용자가 정의한 분할 수준(라벨)에 따라서 이미지를 분할하고, 해당 마스크를 SAM을 통해 정밀하게 조정할 수 있음을 확인함~~<br><br>- 텍스처 편집 구현: <br>~~Text2LIVE로 텍스트 조건 기반 RGBA 편집 레이어 생성 및 적용~~<br>*ControlNet의 inpaint 기능을 이용하여 텍스트 조건 기반*<br>![Image](https://github.com/user-attachments/assets/aa367cc6-309d-4646-b6a0-da0031eaa508)<br>~~→ 사용자가 제공한 텍스쳐 프롬프트가 객체에 적용된 것을 볼 수 있다~~ *→ 마스크로 지정된 영역(흰색으로 표시된 부분)에 사용자가 제공한 텍스트 조건이 적용되어 새로운 텍스쳐로 대체 생성된 것을 확인할 수 있음*<br><br>*1. 사용자 요구사항을 반영한 멀티뷰 생성과 멀티뷰 분할 및 변경<br>Text Prompt Parsing module+Zero-1-to-3/ControlNet<br>*: *사용자의 복합 입력으로부터 객체-부위 정보와, 부위별 스타일 정보 추출. 최종적으로는 2개의 json 파일 추출. Grounded-SAM에서 최적으로 처리되는 형식의 부위-분할 프롬프트, ControlNet의 input으로 처리되는 부위 객체별 스타일 맵핑 프롬프트. ollama를 활용한 프롬프트 엔지니어링 및 파이썬으로 구현 완료*.<br><*Text prompt parsing module- 다중부위를 중심으로 한 예시>*<br>*prompt: a chair with black metalic legs and white furry seat*<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/parsed_prompt.png"><br>- *Grounded-SAM Prompt: legs, seat, chair*<br>- *Style Mapping-ControlNet prompt*<Br>*legs_mask.png → "black metalic"<br>seat_mask.png → "white furry"*<br>*chair_mask.png → " "*<br><br>*2. 멀티뷰 생성 및 뷰 별 분할 수행<br>Image Pre-processing module+Grounded-SAM<br>*: *뷰마다 분할 작업을 위한 전처리. 512/*512 크기의 8개 개별 이미지로 가공. 파이썬 활용 구현 완료*.<br><br>*<Zero-1-to-3 + Image Pre-processing Module 예시>*<br>- input<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/pot_input.png"><br>- Zero-1-to-3 output<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/preprocess_input.png"><br>- Image Pre-processing output<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/preprocess_output.png"><br><br>*Grounded-SAM + Part Instance Information Aggregation Module+Mask aggregation module*<br>: *한번에 탐지되지 못하고 여러개로 나누어진 부위 객체들을 라벨 이름에 따라 하나의 part instance로 통합. 파이썬 활용 구현 완료*. <br><br>모듈을 통해 정리된 정보들을 통해 부위별 마스크 생성. 객체_mask.png 형식. 이렇게 생성된 마스크 이미지는 1번 모듈에서 생성된 객체-스타일 맵핑 정보와 함께 ControlNet에서 사용. 파이썬 활용 구현 완료*.<br>*3. 분할 정보 및 사용자 요구사항 기반 텍스처 편집<br> Part Instance Information Aggregation Module+Mask aggregation module+ ControlNet<br>: 이전 모듈들에서 추출된 정보를 기반으로 부위별 텍스처 변경. 가공된 멀티뷰 이미지와 부위별 마스크 이미지, 부위별 텍스처 정보를 활용해 이미지 부분 변경. 파이썬 활용 구현 완료*.<br>*<Grounded-sam+Part-Information Aggregation Module+Mask aggregation Module>*<br>-*Grounded-sam output*<br>a) image output<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/grounded_sam_output_image.png"><br>b) json output<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/grounded_sam_output_json.png"><br>*<Part-Information Aggregation Module+Mask aggregation module output>*<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/mask_aggregation_output.png"><br><br>*4. 생성된 이미지 정제 및 3D 렌더링<br>Aggregation & Refinement Module+3D Rendering<br>*: *2D 단일 이미지로만 생성된 3D 형태와 편집과정을 거친 2D 멀티 이미지를 비교하여 2D-3D 사이에서 일어나는 불일치 및 오류를 감소시킴. SDS loss 와 3D aware attention을 이용.구현 중. 2D 생성 이미지의 해상도 문제로 3D 활용 렌더링 방식에 문제가 발생하여 결과물을 아직 생성하지 못했고, refinement를 하지 못함. 실험 내용의 경우 5번 기타 항목 참조.*|
| (5) 기타 |~~- Gaussian Splatting과 선정한 렌더링 기법 비교 실험 중<br>- 가구 데이터셋 가공 및 라벨링 작업 진행 중~~ <br>1. *3D 렌더링 기법 재선택<br>- NeRF/Neus 실험 결과 분석<br>a) 학습·추론 시간 비교 (mm:ss)<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/neus_nerf.png"><br>b) 결과 비교<br>NeuS<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/neus.png"><br>NeRF<br><img src="https://github.com/jiu-jung/capstone-JEY/blob/main/images/nerf.png"><br>SyncDreamer가 생성한 256×256 멀티뷰 이미지를 자체 메쉬 추출 스크립트(NeuS·NeRF)로 처리해 본 결과, NeuS는 가장 정교한 메쉬를 생성했지만 학습·추론 시간이 과도하게 길었다. 반면 NeRF는 처리 속도는 훨씬 빠르나, 저해상도 입력 한계로 메쉬에 심한 노이즈가 나타나 품질이 크게 저하되었다. 즉, SyncDreamer 기반 파이프라인은 품질‑속도 균형이 맞지 않아 “가볍고 고화질”이라는 연구 목표를 충족하지 못한다. 이에 우리는 더 높은 해상도 멀티뷰를 수 분 내 생성할 수 있는 Zero123로 전환해, 경량·고품질 워크플로우를 신속히 구축할 예정이다.<br><br>2. refinement module 구현 중<br><br>3. Grounded-SAM 성능 향상을 위한 프로세스 도입*|

<br>
