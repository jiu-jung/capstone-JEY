# 1차보고서

Status: In Progress
Due: March 31, 2025
Project:  G_중간발표 전 (https://www.notion.so/G_-1ae70dd6aba280248474ef9fd3508576?pvs=21)
Priority: High
Tags: 제출

- 유광현 교수님 설명
    
    [유광현 교수.txt](%EC%9C%A0%EA%B4%91%ED%98%84_%EA%B5%90%EC%88%98.txt)
    

![image.png](image.png)

**Team-Info**

| (1) 과제명 | Lightweight Part-Aware 3D Texture Editing Using 2D Stable Diffusion |
| --- | --- |
| (2) 팀 번호 / 팀 이름 | 19-JEY |
| (3) 팀 구성원 | 이다예 (1917026): 리더, 아이디어 및 프로젝트 계획 관리, 2D-to-3D adaptation, 2D semantic segmentation 연구 및 개발, part segmentation 검증
**이상은 (2171036): 팀원, 회의/면담 기록 정리 및 관리, Text to texture 연구 및 개발, 2D-to-3D adaptation, 2D semantic segmentation 검증
**정지우 (2171046): 팀원, 프로젝트 마일스톤 관리 및 공유 파일 관리, part segmentation 연구 및 개발, Text to texture 검증 |
| (4) 팀 지도교수 | 김영준 교수 |
| (5) 과제 분류 | 연구 |
| (6) 과제 키워드 | Semantic Part Segmentation, Controllable Diffusion, Texture generation |
| (7) 과제 내용 요약 |   본 연구는 고사양 3D 모델링 기술 없이도 특정 부위의 텍스처를 정밀하게 편집할 수 있는 2D 기반의 경량화된 텍스처링 프로세스를 제안한다. SyncDreamer를 활용하여 단일 이미지로부터 멀티뷰를 생성하고, VLPart와 SAM을 통해 2D 이미지에서 의미 기반 부위 분할을 수행한다. 이후 Text2LIVE를 활용해 텍스트 조건에 맞는 부위별 텍스처 편집을 적용한다. Aggregation 모듈을 통해 시점 간 마스크를 통합하고, refinement 단계에서 SyncDreamer 기반 정렬을 수행해 정확성을 높인다. 최종적으로 부위별 제어 정확도와 텍스처 품질을 동시에 확보하면서도 일반 사용자가 쉽게 접근할 수 있는 시스템을 구축하는 것을 목표로 한다. |

**Project-Summary**

| 항목 | 내용 |
| --- | --- |
| (1) 문제 정의 |   최근 XR, AR, VR 등 3D 환경 기반 기술의 수요가 증가함에 따라, 이를 구성하는 핵심 요소인 3D 모델의 중요성도 함께 부각되고 있다. 특히 사용자의 요구에 맞춘 3D 모델 생성 및 편집 기술이 활발히 연구되면서, 이를 상용화한 다양한 서비스도 등장하고 있다. NeRF나 Gaussian Splatting과 같은 최신 렌더링 기법의 적용으로 성능은 크게 향상되었지만, 여전히 텍스처 오류가 존재하며, 사용자의 세부 요구사항을 정확히 반영하는 데에는 한계가 있다. 이러한 문제는 텍스처 작업에 익숙한 전문가가 아니라면 해결하기 어렵다는 점에서, 일반 사용자에게는 높은 진입 장벽으로 작용한다.
  특히 실제 3D 응용 프로그램이나 애니메이션을 제작할 때는 모델 전체가 아닌 특정 부위 단위의 정밀한 제어나 상호작용이 필요하며, 이를 위해서는 3D 모델을 부위별로 분리(segment)하는 작업이 필수적이다. 그러나 이 과정 역시 고난이도의 작업으로, 대부분의 일반 사용자에게는 익숙하지 않으며 전문가의 도움이 요구된다. 최근에는 이러한 문제를 해결하기 위해 3D part segmentation 분야에서 다양한 연구가 이루어지고 있지만, 많은 연구들이 고사양 하드웨어와 복잡한 3D 데이터를 전제로 하기 때문에 상용화에는 여전히 제약이 따른다.
  이러한 배경 속에서, 우리는 일반 사용자도 손쉽게 사용할 수 있는 **2D 기반 부위별 텍스처 편집 방식**을 제안한다. 이는 기존의 복잡하고 고사양을 요구하는 3D 텍스처링 기법의 한계를 극복하고, 부위별 정밀 제어가 가능한 사용자 친화적인 솔루션으로서의 가능성을 가진다. |
| (2) 기존연구와의 비교 | **1. 객체 분할 (Object Segmentation)**
**기존 연구 1 – PointNet (Qi et al., 2017)**
PointNet은 3D 포인트 클라우드를 입력으로 하여 부위 단위의 정밀한 분할을 수행한 대표적인 fully-supervised 방식이다. 이는 3D 구조를 직접 다루어 정확한 분석이 가능하다는 장점이 있지만, 라벨링 비용이 높고 도메인 특화 경향이 강하며 일반화에 어려움이 있다는 단점이 있다.
→ **우리 연구는** 이러한 한계를 보완하기 위해 2D Vision Language Model(VLM)을 기반으로 하여 연산량이 적으며 빠르게 동작할 수 있도록 접근하였다.

**2. 2D to 3D Adaptation**
**기존 연구 2 – PartSLIP (Liu et al., 2022)**
PartSLIP은 2D에서 GLIP을 활용하여 부위 인식을 수행한 후, 다양한 시점에서 얻은 bounding box 결과를 집계하여 3D 포인트 클라우드에 대응시킨다. aggregation과 voting 알고리즘을 통해 신뢰도를 향상시킨 것이 특징이다.
→ **우리 연구는** PartSLIP의 voting 구조를 기반으로 하면서도, 신경망을 통해 가중치 예측을 강화한 PartSTAD 방식을 적용해 예측 신뢰성을 높였다.

**3. 3D 텍스처 편집 (3D Texture Editing)**
**기존 연구 3 – TEXTure (Richardson et al., 2023), Text2Tex (Chen et al., 2023)**
두 연구 모두 stable diffusion 기반의 depth-aware 모델을 활용하여 텍스트 프롬프트에 따라 다양한 시점에서 일관성 있는 텍스처 맵을 생성한다. 그러나 조명 정보가 반영된 텍스처 생성으로 인해 3D 환경에서는 그림자나 반사 등 문제로 품질 저하가 발생한다. 또한 부위별 조건을 부여할 경우 왜곡된 결과가 도출된다.
TEXTure를 실험하여, 텍스처 맵 생성 결과가 높은 완성도로 생성됨을 확인하였지만, 부위별 인식이 불가함을 확인하였다. 

**기존 연구 4 – Paint3D (Zeng et al., 2024), Paint-it (Kim et al., 2024)**
두 연구는 PBR 호환 텍스처 생성을 통해 실제 렌더링 품질 문제를 해결한다. 특히 Paint3D는 depth-aware diffusion과 UV inpainting 기법을 조합하여 조명 없는 고해상도 텍스처 생성을 가능하게 한다.
우리는 Paint3D 실험을 통해 조명 문제 없는 성공적인 텍스처 생성 결과를 확인 했으나, 텍스트 프롬프트만으로 완전한 부위별 텍스처 생성에는 한계가 있음을 확인하였다.

→ 기존 연구 3, 기존 연구 4에 대해, 우리 연구는 텍스처 편집 단계 전 2D 기반 세그멘테이션 모델을 활용해 의미론적 파트 분할을 수행하여 부위별 인식을 통한 텍스처 편집이 가능하도록 하였다. |
| (3) 제안 내용 |   정의한 문제를 해결하기 위해 접근성이 높은 2D 기반의 부위별 텍스처링 프로세스를 제안한다. 이를 통해 사용자가 원하는 부위별 텍스처를 손쉽게 생성하고, 보다 향상된 사용자 경험을 제공하고자 한다.
  SyncDreamer로 생성한 멀티뷰 이미지에 SAM과 VLPart를 이용해 부위별 마스크를 생성하고, Text2LIVE 기반의 텍스처 편집을 수행한 후, Aggregation module을 통해 정보를 통합하고 refinement 과정에서 2D-3D 간 시점 정렬 및 정확도 비교를 통해 최종 텍스처 결과물을 생성한다.
  이를 정리한 전체 파이프라인은 다음과 같으며, 각 단계에 대한 구체적 설명은 **(5) 주요 기능 리스트**에서 이어진다. 
<사진>
**Input → SyncDreamer → Segmentation(VLPart + SAM) → Texture Editing (Text2LIVE) → Aggregation → Refinement** |
| (4) 기대효과 및 의의 |   고사양 3D 모델링 환경 없이도 정밀한 부위별 텍스처 편집이 가능하며, 사용자 친화적이면서도 높은 품질의 결과물을 생성할 수 있다. 이는 XR/VR 콘텐츠 제작, 사용자 맞춤형 가구 디자인, 메타버스 3D asset 제작 등에서 활용 가능하다. |
| (5) 주요 기능 리스트 | **1. SyncDreamer:** Multiview 생성
기존 연구들을 비교 분석한 결과, 텍스처 품질 측면에서 **SyncDreamer**가 가장 우수한 성능을 보였다. 따라서, **Refinement 단계에서 사용할 단일 이미지 기반 3D 모델의 품질을 보장하기 위해 SyncDreamer를 채택한다.**

**2. VLPart + SAM:** Segmentation
생성된 멀티뷰에 대해 **2D 기반 세그멘테이션 모델을 활용**해 의미론적 파트 분할을 수행한다.
• **1차 분할**: VLPart를 활용하여 semantic 정보를 기반으로 초기 분할을 수행
• **2차 정제**: SAM을 활용해 경계 분할 성능을 높여 전체 정확도를 향상
이러한 **다중 분할 후 통합하는 과정을** 통해 semantic 표현력과 경계의 정밀도를 동시에 확보한다.

**3. Text2LIVE:** Texture Editing
분할된 결과에 대해 **Text2LIVE**를 이용해 텍스처 편집을 수행한다.
이 단계에서는 입력 텍스트 정보에 따라 부위별로 자연스럽고 일관성 있는 텍스처를 생성한다.

**4. Aggregation & Refinement**
기존 연구에서 지적되었듯이, **2D 정보만으로는 정확한 3D 형태 복원에 한계**가 있다. 따라서, 3D 구조 정보를 최대한 보존하기 위해 **2D-to-3D 적응(adaptation)** 기법의 개선을 목표로 설정한다. 그러나, 복잡한 MLP나 과도한 모듈 추가는 연구 목적과 부합하지 않기 때문에, **전체 프로세스에 3D 정보를 무리하게 삽입하지 않고**, 최종 **Refinement 단계에서 3D-aware attention 기법을 도입한다. 해당 과정에서는 변형된 단일 이미지로 생성된 3D 모델의 다각도 이미지와, 변형된 멀티뷰 이미지들을 비교하여 텍스처를 보정한다.** 또한, **SDS loss 기반의 3D-aware attention**을 활용하여 **전체 객체 수준의 정확성과 품질을 향상**시킨다. |

**Project-Design & Implementation**

| 항목 | 내용 |
| --- | --- |
| (1) 요구사항 정의 | - **기능 요구사항**:
① SyncDreamer 멀티뷰 생성
② VLPart + SAM 기반 2D segmentation
③ Text2LIVE를 활용한 부위별 텍스처 편집
④ Aggregation module 설계 및 해당 데이터 기반 3D rendering 
⑤ 3D/2D 결합 refinement

- **데이터 요구사항**:
가구 중심의 라벨링 이미지 (Pascal-Part, ADE20K 등 가공), 3D 렌더링 결과 비교용 ground truth |
| (2) 전체 시스템 구성 | - **모듈 구조**:
<사진>
Input → SyncDreamer → Segmentation(VLPart + SAM) → Texture Editing (Text2LIVE) → Aggregation → Refinement

1. **SyncDreamer:** SyncDreamer를 활용해 단일 이미지 기반 멀티뷰 생성
2. **VLPart + SAM:** VLPart → SAM 순으로 input 이미지에 텍스트 프롬프트에서 주어진 부위 정보를 기반으로 하여 분할을 수행하고, 해당 결과를 SAM을 이용하여 정교화.
3. **Text2LIVE:** 분할된 마스크를 기반으로, Text2LIVE를 활용해 부위별 텍스트 조건 기반 특정 부위 텍스처 편집
4. **Aggregation & Refinement**: 2D 단일 이미지로만 생성된 3D 형태와 편집과정을 거친 2D 멀티 이미지를 비교하여 2D-3D 사이에서 일어나는 불일치 및 오류를 감소시킴. SDS loss 와 3D aware attention을 이용.

- **외부 모듈**:
SyncDreamer (multiview generation), SAM (segmentation), Text2LIVE (texture editing) |
| (3) 주요엔진 및 기능 설계 | - **Multiview Generator**: SyncDreamer를 통해 8-view 동기화 이미지 생성
- **Segmenter**: VLPart로 의미 정보 분할 후 SAM으로 경계 보정
- **Texturing Module**: Text2LIVE를 이용해 텍스트 기반 RGBA 편집
- **Aggregation & Refinement**: 다양한 시점 마스크 통합, SyncDreamer 시점 보정 활용 |
| (4) 주요 기능의 구현 | - **부위 분할**: 
VLPart + SAM 순차 적용하여 부위별 마스크 생성
<사진>
→ 사용자가 정의한 분할 수준(라벨)에 따라서 이미지를 분할하고, 해당 마스크를 SAM을 통해 정밀하게 조정할 수 있음을 확인함.

- **텍스처 편집 구현**: 
Text2LIVE로 텍스트 조건 기반 RGBA 편집 레이어 생성 및 적용
<사진>
→ 사용자가 제공한 텍스쳐 프롬프트가 객체에 적용된 것을 볼 수 있다. |
| **(5) 기타** | - Gaussian Splatting과 선정한 렌더링 기법 비교 실험 중
- 가구 데이터셋 가공 및 라벨링 작업 진행 중 |

---

- 전체 시스템 구성

### Part Segmentation

segmentation prompt <bicycle tier, bicycle body, bicycle seat, bicycle head>

**입력 이미지**

![bicycle_2.jpg](bicycle_2.jpg)

**VLPart** (with prompt)

![1.jpg](1.jpg)

**SAM (**without prompt)

![bicycle_2_1.png](bicycle_2_1.png)

![bicycle_1.jpg](bicycle_1.jpg)

![2.jpg](2.jpg)

![bicycle_1_1.png](bicycle_1_1.png)

Text2LIVE

**입력 이미지**

![horse.jpg](horse.jpg)

prompt: **“golden horse”**

![composite.png](composite.png)

prompt: **“wooden horse”**

![composite.png](composite%201.png)

- 실험결과 archive
    - 원본 이미지
        
        ![horse.jpg](horse.jpg)
        
    1. golden horse
    
    screen_text: "golden horse" # texts, describing the edit layer
    comp_text: "golden horse" # texts, describing the full edited image
    src_text: "horse" # texts, describing the input image
    
    - 결과
        - edit
            
            ![edit.png](edit.png)
            
        - composite
            
            ![composite.png](composite.png)
            
    1. wooden horse
    
    screen_text: "wooden horse" # texts, describing the edit layer
    comp_text: "wooden horse" # texts, describing the full edited image
    src_text: "horse" # texts, describing the input image
    
    - 결과
        - edit
            
            ![edit.png](edit%201.png)
            
        - composite
            
            ![composite.png](composite%201.png)